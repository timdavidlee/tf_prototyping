{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41534364.0\n",
      "43528080.0\n",
      "54417252.0\n",
      "57864344.0\n",
      "42900344.0\n",
      "20122392.0\n",
      "7109028.5\n",
      "2866950.5\n",
      "1674963.5\n",
      "1239862.8\n",
      "1002565.2\n",
      "835844.6\n",
      "706569.75\n",
      "602788.1\n",
      "517806.94\n",
      "447703.34\n",
      "389237.6\n",
      "340099.66\n",
      "298489.2\n",
      "263026.5\n",
      "232627.92\n",
      "206462.58\n",
      "183817.38\n",
      "164132.81\n",
      "146958.81\n",
      "131921.45\n",
      "118703.86\n",
      "107050.65\n",
      "96735.016\n",
      "87582.77\n",
      "79475.61\n",
      "72258.11\n",
      "65803.2\n",
      "60025.5\n",
      "54838.17\n",
      "50173.742\n",
      "45966.484\n",
      "42163.203\n",
      "38718.58\n",
      "35593.023\n",
      "32756.414\n",
      "30173.463\n",
      "27820.447\n",
      "25672.947\n",
      "23711.078\n",
      "21917.5\n",
      "20275.785\n",
      "18770.684\n",
      "17389.648\n",
      "16120.895\n",
      "14954.436\n",
      "13881.654\n",
      "12893.65\n",
      "11982.805\n",
      "11141.07\n",
      "10364.543\n",
      "9646.854\n",
      "8983.674\n",
      "8370.158\n",
      "7802.453\n",
      "7276.779\n",
      "6789.928\n",
      "6338.4517\n",
      "5918.465\n",
      "5528.6113\n",
      "5166.669\n",
      "4830.361\n",
      "4517.6934\n",
      "4226.9727\n",
      "3956.4053\n",
      "3704.471\n",
      "3469.912\n",
      "3251.3403\n",
      "3047.5293\n",
      "2857.4395\n",
      "2680.212\n",
      "2514.8296\n",
      "2360.3804\n",
      "2216.3794\n",
      "2081.7852\n",
      "1955.9368\n",
      "1838.2156\n",
      "1728.0977\n",
      "1625.106\n",
      "1528.6711\n",
      "1438.3301\n",
      "1353.7097\n",
      "1274.4304\n",
      "1200.081\n",
      "1130.362\n",
      "1064.9528\n",
      "1003.55896\n",
      "945.93884\n",
      "891.885\n",
      "841.07764\n",
      "793.34155\n",
      "748.48737\n",
      "706.3137\n",
      "666.6759\n",
      "629.40247\n",
      "594.33246\n",
      "561.3327\n",
      "530.26874\n",
      "501.02472\n",
      "473.48727\n",
      "447.5539\n",
      "423.13086\n",
      "400.10492\n",
      "378.4041\n",
      "357.95184\n",
      "338.67126\n",
      "320.48685\n",
      "303.32797\n",
      "287.1312\n",
      "271.85284\n",
      "257.42792\n",
      "243.80782\n",
      "230.94614\n",
      "218.79858\n",
      "207.33044\n",
      "196.48886\n",
      "186.23865\n",
      "176.55482\n",
      "167.40445\n",
      "158.75055\n",
      "150.56125\n",
      "142.81523\n",
      "135.48648\n",
      "128.55618\n",
      "121.99304\n",
      "115.78121\n",
      "109.90204\n",
      "104.33559\n",
      "99.06659\n",
      "94.07237\n",
      "89.34135\n",
      "84.85936\n",
      "80.612885\n",
      "76.58593\n",
      "72.77196\n",
      "69.15546\n",
      "65.72442\n",
      "62.474888\n",
      "59.38891\n",
      "56.46284\n",
      "53.68676\n",
      "51.053932\n",
      "48.556034\n",
      "46.182705\n",
      "43.932972\n",
      "41.79648\n",
      "39.76639\n",
      "37.84001\n",
      "36.00923\n",
      "34.272297\n",
      "32.622135\n",
      "31.052841\n",
      "29.563097\n",
      "28.147171\n",
      "26.802273\n",
      "25.523209\n",
      "24.308289\n",
      "23.152067\n",
      "22.053783\n",
      "21.008057\n",
      "20.015438\n",
      "19.069729\n",
      "18.170963\n",
      "17.316248\n",
      "16.502846\n",
      "15.728752\n",
      "14.991543\n",
      "14.291239\n",
      "13.62401\n",
      "12.989367\n",
      "12.385311\n",
      "11.809738\n",
      "11.261792\n",
      "10.740536\n",
      "10.243136\n",
      "9.770569\n",
      "9.3204\n",
      "8.890842\n",
      "8.482122\n",
      "8.09254\n",
      "7.721201\n",
      "7.367944\n",
      "7.030571\n",
      "6.709906\n",
      "6.4036164\n",
      "6.112193\n",
      "5.833973\n",
      "5.5691276\n",
      "5.316328\n",
      "5.075297\n",
      "4.8456507\n",
      "4.6263285\n",
      "4.417843\n",
      "4.218293\n",
      "4.0282316\n",
      "3.847105\n",
      "3.674232\n",
      "3.5091562\n",
      "3.352081\n",
      "3.2020726\n",
      "3.0586512\n",
      "2.921871\n",
      "2.7914367\n",
      "2.6668668\n",
      "2.5480733\n",
      "2.434622\n",
      "2.3264847\n",
      "2.223166\n",
      "2.1244776\n",
      "2.030284\n",
      "1.9404734\n",
      "1.854584\n",
      "1.7725983\n",
      "1.6944106\n",
      "1.6196692\n",
      "1.5481336\n",
      "1.4799759\n",
      "1.4148343\n",
      "1.3526994\n",
      "1.2932386\n",
      "1.2364078\n",
      "1.1823065\n",
      "1.1304922\n",
      "1.081023\n",
      "1.0337474\n",
      "0.9885851\n",
      "0.9454112\n",
      "0.9041934\n",
      "0.8647741\n",
      "0.8271519\n",
      "0.79108524\n",
      "0.7567816\n",
      "0.7238618\n",
      "0.6924218\n",
      "0.66238815\n",
      "0.633705\n",
      "0.6061631\n",
      "0.57994545\n",
      "0.5548799\n",
      "0.530925\n",
      "0.5079247\n",
      "0.485938\n",
      "0.46495157\n",
      "0.44491\n",
      "0.42577964\n",
      "0.40742457\n",
      "0.3899807\n",
      "0.37326506\n",
      "0.3571623\n",
      "0.341846\n",
      "0.3271875\n",
      "0.3130925\n",
      "0.29971024\n",
      "0.2868815\n",
      "0.27452976\n",
      "0.2628065\n",
      "0.25158823\n",
      "0.24079359\n",
      "0.23054256\n",
      "0.22069398\n",
      "0.21130477\n",
      "0.20228796\n",
      "0.19365415\n",
      "0.18538286\n",
      "0.17753708\n",
      "0.16993636\n",
      "0.16268182\n",
      "0.15577796\n",
      "0.1491904\n",
      "0.14284949\n",
      "0.1367872\n",
      "0.13098735\n",
      "0.1253989\n",
      "0.12012621\n",
      "0.11503819\n",
      "0.110187\n",
      "0.10550391\n",
      "0.101037085\n",
      "0.09676959\n",
      "0.092691794\n",
      "0.08877225\n",
      "0.0850569\n",
      "0.08144608\n",
      "0.07800561\n",
      "0.07473414\n",
      "0.07158473\n",
      "0.068562046\n",
      "0.06566723\n",
      "0.06289908\n",
      "0.06027126\n",
      "0.057732932\n",
      "0.055323884\n",
      "0.052998524\n",
      "0.05078565\n",
      "0.048638053\n",
      "0.046602502\n",
      "0.04465675\n",
      "0.0427894\n",
      "0.040997773\n",
      "0.039269235\n",
      "0.037643917\n",
      "0.036079627\n",
      "0.034584574\n",
      "0.03314275\n",
      "0.03176256\n",
      "0.030441113\n",
      "0.029151758\n",
      "0.027945518\n",
      "0.026799226\n",
      "0.02569684\n",
      "0.024629219\n",
      "0.023603335\n",
      "0.02262126\n",
      "0.021694567\n",
      "0.02080266\n",
      "0.019946786\n",
      "0.01911848\n",
      "0.018327333\n",
      "0.017582549\n",
      "0.016856382\n",
      "0.016161488\n",
      "0.015509904\n",
      "0.014869614\n",
      "0.014266231\n",
      "0.01368337\n",
      "0.013119959\n",
      "0.012587528\n",
      "0.012076473\n",
      "0.011583\n",
      "0.011115208\n",
      "0.010666677\n",
      "0.010241133\n",
      "0.009825289\n",
      "0.009427344\n",
      "0.009047287\n",
      "0.008685775\n",
      "0.00833508\n",
      "0.008000271\n",
      "0.0076843374\n",
      "0.0073799966\n",
      "0.007091199\n",
      "0.0068141045\n",
      "0.006538924\n",
      "0.006283175\n",
      "0.006031906\n",
      "0.005794528\n",
      "0.0055751544\n",
      "0.0053571966\n",
      "0.0051480313\n",
      "0.0049457364\n",
      "0.0047575645\n",
      "0.004577837\n",
      "0.0044012014\n",
      "0.0042348118\n",
      "0.0040739095\n",
      "0.003917747\n",
      "0.003772215\n",
      "0.0036335785\n",
      "0.0034982252\n",
      "0.0033622936\n",
      "0.0032414936\n",
      "0.003123379\n",
      "0.0030067894\n",
      "0.0028960307\n",
      "0.0027922068\n",
      "0.002690366\n",
      "0.0025915913\n",
      "0.0024978924\n",
      "0.0024072642\n",
      "0.0023227178\n",
      "0.0022427067\n",
      "0.0021622062\n",
      "0.0020870091\n",
      "0.002014453\n",
      "0.0019444446\n",
      "0.001879114\n",
      "0.0018141554\n",
      "0.0017532962\n",
      "0.0016972804\n",
      "0.0016390502\n",
      "0.0015820586\n",
      "0.0015284223\n",
      "0.0014771328\n",
      "0.0014298237\n",
      "0.0013840155\n",
      "0.0013388183\n",
      "0.0012932406\n",
      "0.0012524787\n",
      "0.001215236\n",
      "0.0011753485\n",
      "0.0011372191\n",
      "0.0011044289\n",
      "0.0010700349\n",
      "0.001036257\n",
      "0.001006996\n",
      "0.00097391376\n",
      "0.00094566576\n",
      "0.0009168262\n",
      "0.000889664\n",
      "0.00086414756\n",
      "0.0008398971\n",
      "0.0008139862\n",
      "0.00078923\n",
      "0.00076649454\n",
      "0.0007442618\n",
      "0.00072451506\n",
      "0.00070300506\n",
      "0.0006833526\n",
      "0.00066584046\n",
      "0.00064591167\n",
      "0.00062905054\n",
      "0.00061049085\n",
      "0.0005939498\n",
      "0.00057944714\n",
      "0.00056248694\n",
      "0.0005474348\n",
      "0.00053365075\n",
      "0.0005199949\n",
      "0.0005065375\n",
      "0.000493981\n",
      "0.00048155978\n",
      "0.00046923294\n",
      "0.000456443\n",
      "0.00044542993\n",
      "0.0004344815\n",
      "0.00042367505\n",
      "0.0004125773\n",
      "0.00040331666\n",
      "0.0003932718\n",
      "0.00038372498\n",
      "0.00037476665\n",
      "0.0003664051\n",
      "0.00035695438\n",
      "0.0003485495\n",
      "0.000340493\n",
      "0.00033254785\n",
      "0.0003252819\n",
      "0.0003182984\n",
      "0.00031157586\n",
      "0.00030498422\n",
      "0.0002979379\n",
      "0.00029148159\n",
      "0.0002852184\n",
      "0.00027829217\n",
      "0.00027232873\n",
      "0.00026671437\n",
      "0.00026184303\n",
      "0.000255537\n",
      "0.0002502096\n",
      "0.00024550504\n",
      "0.0002397221\n",
      "0.00023490045\n",
      "0.00023030576\n",
      "0.0002252206\n",
      "0.00022049759\n",
      "0.0002159914\n",
      "0.00021167961\n",
      "0.00020777574\n",
      "0.00020410071\n",
      "0.00020000734\n",
      "0.00019596805\n",
      "0.0001928829\n",
      "0.0001891748\n",
      "0.00018492679\n",
      "0.00018158034\n",
      "0.00017778696\n",
      "0.0001742648\n",
      "0.00017099522\n",
      "0.00016776353\n",
      "0.00016485897\n",
      "0.00016263672\n",
      "0.00015947018\n",
      "0.00015674622\n",
      "0.0001539803\n",
      "0.00015122294\n",
      "0.00014869613\n",
      "0.0001460283\n",
      "0.00014365012\n",
      "0.00014119463\n",
      "0.00013882734\n",
      "0.00013584386\n",
      "0.00013399283\n",
      "0.00013163054\n",
      "0.00012937534\n",
      "0.00012728997\n",
      "0.0001252002\n",
      "0.00012319307\n",
      "0.00012139334\n",
      "0.00011916176\n",
      "0.00011748592\n",
      "0.000116263654\n",
      "0.00011398684\n",
      "0.00011205144\n",
      "0.00011034035\n",
      "0.000108199456\n"
     ]
    }
   ],
   "source": [
    "# Code in file autograd/tf_two_layer_net.py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# First we set up the computational graph:\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create placeholders for the input and target data; these will be filled\n",
    "# with real data when we execute the graph.\n",
    "x = tf.placeholder(tf.float32, shape=(None, D_in))\n",
    "y = tf.placeholder(tf.float32, shape=(None, D_out))\n",
    "\n",
    "# Create Variables for the weights and initialize them with random data.\n",
    "# A TensorFlow Variable persists its value across executions of the graph.\n",
    "w1 = tf.Variable(tf.random_normal((D_in, H)))\n",
    "w2 = tf.Variable(tf.random_normal((H, D_out)))\n",
    "\n",
    "# Forward pass: Compute the predicted y using operations on TensorFlow Tensors.\n",
    "# Note that this code does not actually perform any numeric operations; it\n",
    "# merely sets up the computational graph that we will later execute.\n",
    "h = tf.matmul(x, w1)\n",
    "h_relu = tf.maximum(h, tf.zeros(1))\n",
    "y_pred = tf.matmul(h_relu, w2)\n",
    "\n",
    "# Compute loss using operations on TensorFlow Tensors\n",
    "loss = tf.reduce_sum((y - y_pred) ** 2.0)\n",
    "\n",
    "# Compute gradient of the loss with respect to w1 and w2.\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])\n",
    "\n",
    "# Update the weights using gradient descent. To actually update the weights\n",
    "# we need to evaluate new_w1 and new_w2 when executing the graph. Note that\n",
    "# in TensorFlow the the act of updating the value of the weights is part of\n",
    "# the computational graph; in PyTorch this happens outside the computational\n",
    "# graph.\n",
    "learning_rate = 1e-6\n",
    "new_w1 = w1.assign(w1 - learning_rate * grad_w1)\n",
    "new_w2 = w2.assign(w2 - learning_rate * grad_w2)\n",
    "\n",
    "# Now we have built our computational graph, so we enter a TensorFlow session to\n",
    "# actually execute the graph.\n",
    "with tf.Session() as sess:\n",
    "  # Run the graph once to initialize the Variables w1 and w2.\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "\n",
    "  # Create numpy arrays holding the actual data for the inputs x and targets y\n",
    "  x_value = np.random.randn(N, D_in)\n",
    "  y_value = np.random.randn(N, D_out)\n",
    "  for _ in range(500):\n",
    "    # Execute the graph many times. Each time it executes we want to bind\n",
    "    # x_value to x and y_value to y, specified with the feed_dict argument.\n",
    "    # Each time we execute the graph we want to compute the values for loss,\n",
    "    # new_w1, and new_w2; the values of these Tensors are returned as numpy\n",
    "    # arrays.\n",
    "    loss_value, _, _ = sess.run([loss, new_w1, new_w2],\n",
    "                                feed_dict={x: x_value, y: y_value})\n",
    "    print(loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
